---
title: 'STOR 455 Homework #3'
subtitle: "40 points - Due 2/21 at 5:00pm"
output:
  word_document: default
  pdf_document: default
---

__Directions:__ You will be assigned to a group of four to five students for this assignment. Parts 1, 2, 3, & 6 should be submitted individually to Gradescope by each student in your group (although you should work together on these parts). Parts 4 & 5 should be submitted as a group to Gradescope. There are separate places to submit the individual and group portions of the assignment. 
    
__Situation:__ Can we predict the selling price of a house in Ames, Iowa based on recorded features of the house? That is your task for this assignment. Each group will have a dataset with information on forty potential predictors and the selling price (in $1,000’s) for a sample of homes. The data set for your group is in AmesTrain??.csv (where ?? corresponds to your group number) and can be found in the AmesTrain zipped file under class 14 in Canvas. A separate file identifies the variables in the Ames Housing data and explains some of the coding.

### Part 1. Build an initial “basic” model    
Your basic model can use any of the quantitative variables in the dataset but should NOT use the categorical variables or transformations. Use your data to select a set of predictors to include in your model, utilizing at least two model selection methods (e.g. don’t just check all subsets, although it will work well here, this method will fail in future assignments). You **should not** show all of the output for every model you consider, but you should give a clear description of the path you took and the criteria that you used to compare competing models.

Include the following information for this initial choice of a model:

* the summary() output for your model
* comments on which (if any) of the predictors in the model are not significant at a 5% level
* comments on what the VIF values tell you about the individual predictors in your model
    
Do not consider the Order variable (that is just an observation number) as one of your predictors. Avoid predictors that are exactly related. For example, if GroundSF=FirstSF+SecondSF you will likely get trouble if you try to put all three in the same model. 
```{r} 
library(tidyverse)
library(Stat2Data)
source("https://raw.githubusercontent.com/JA-McLean/STOR455/master/scripts/ShowSubsets.R")
library(car)
library(leaps)
library(readr)

AmesData <- read.csv('AmesTrain3.csv')
AmesData1 <- select(AmesData, is.numeric)
AmesData1 <- select(AmesData1, -Order)
AmesModFull <- lm(Price ~ ., AmesData1)
MSE <- (summary(AmesModFull)$sigma)^2
AmesModNone <- lm(Price ~1, AmesData1)
simple_model <- step(AmesModNone, scope = list(upper = AmesModFull), scale = MSE, trace = FALSE)
#all <- regsubsets(Price ~., data = AmesData1, nvmax = 15)
#print.data.frame(ShowSubsets(all))

```
```{r}
fullmodel <- lm(Price ~ Quality + GroundSF + BasementFinSF + LotArea + YearBuilt + GarageSF + BasementSF + YearRemodel + LotFrontage + Fireplaces + HalfBath + Condition + Bedroom + TotalRooms + ScreenPorchSF + EnclosedPorchSF, data = AmesData1)
reducedmodel <- lm(Price ~ Quality + GroundSF + BasementFinSF + LotArea + YearBuilt + GarageSF + BasementSF + YearRemodel + LotFrontage + Fireplaces + Condition + Bedroom + TotalRooms, data = AmesData1)
anova(fullmodel, reducedmodel)
```

```{r}
summary(reducedmodel)
vif(reducedmodel)
```
#We decided to use a model with 13 predictors. Initially, we used ShowSubsets() to explore the best model for various predictor counts, considering adjusted R squared and Mallow's Cp for each setup. Despite this, we still weren't sure about the ideal number of predictors, so we turned to stepwise regression. This method checks if it's better to keep or drop existing predictors. In the end, we went with the 17-predictor model, as determined by stepwise regression, which aligned with the potential range of models from the first method.

#Although this model had a pretty good adjusted R squared (0.8603), we noticed that FirstSF and SecondSF had high VIF values, likely because they're closely related. To simplify things, we replaced these predictors with just GroundSF, resulting in a 16-predictor model. However, upon closer inspection, three predictors in this model weren't statistically significant at the 0.05 level. To decide between the reduced 13-predictor and full 16-predictor models, we ran an F-test. The result, a p-value of 0.078, didn't support rejecting the null hypothesis in favor of the reduced model. So, we stuck with the reduced model as our final choice.

#In the final model, all predictors were significant at the 0.05 level, and their VIF values were below 5. GroundSF was the highest at 4.697278, suggesting some correlation with other predictors but not enough to worry about multicollinearity.


### Part 2. Residual analysis for your "basic" model    
Perform a residual analysis for the model you chose in Part 1. Include any plots relevant to checking model conditions - with interpretations. Also check whether any of the data cases are unusual with respect to standardized/studentized residuals. Since there are a lot of data points don’t worry about the “mild” cases for residuals, but indicate what specific criteria you are using to identify “unusual” points. 
   
Adjust your model (either the predictors included or data values that are used to fit it, but not yet using transformations) on the basis of your residual analysis – but don’t worry too much about trying to get all of the conditions “perfect”.  For example, don’t automatically just delete any points that might give large residuals! If you do refit something, be sure to document what changed and include the new summary() output.

```{r}
plot(simple_model)

rstandard_data <- abs(rstandard(simple_model)) > 3
#sum(abs(rstandard(simple_model)) > 3)
rstudent_data <- abs(rstudent(simple_model)) > 2
#rstandard_data
#rstudent_data
AmesData2 <- AmesData1[!rstandard_data, ]
```

#So, we will first interpret these various residual plots. The normal quantile plot indicates that the residuals roughly conform to a normal distribution, albeit with some notably extreme values at indices 179, 222, and 343. This suggests that while the distribution of residuals possesses long tails on both ends, it does not appear to be heavily skewed, given the presence of both large and small residuals. Moving on to the residuals vs. fitted plot, we observe a curved pattern rather than a linear one, indicating that a linear model might not adequately capture the relationship between these variables. Similarly, the scale-location plot also displays a curved trend, coupled with varying spread in the residuals vs. fitted plot, suggesting a lack of constant variance among the residuals. Lastly, the residuals vs. leverage plot identifies observation number 179 as lying beyond Cook's Distance, with two others (343 and 222) in close proximity. Therefore, observation 179 emerges as the sole significantly influential data point.

#So, I focused  on data points with standardized residuals above three. Among these, the corresponding studentized residuals were all above two. Consequently, we excluded the those massivly large residuals from the dataset, as they were deemed outliers based on these criteria.



```{r}
AmesModFull2 <- lm(Price ~ ., AmesData2)
MSE2 <- (summary(AmesModFull)$sigma)^2
AmesModNone2 <- lm(Price ~1, AmesData2)
simple_model2 <- step(AmesModNone2, scale = MSE2, scope = list(upper = AmesModFull2), trace = FALSE)
summary(simple_model2)
```

#With the new dataframe, excluding observations associated with the largest residuals, we decided to re-run stepwise regression to assess its impact on the predictors.

```{r}
fullmodel1 <- lm(formula = Price ~ Quality + GroundSF + BasementFinSF + BasementSF + 
    LotArea + YearBuilt + GarageSF + YearRemodel + Bedroom + 
    LotFrontage + FullBath + Fireplaces + Condition, data = AmesData2)
reducedmodel1 <- lm(formula = Price ~ Quality + GroundSF + BasementFinSF + BasementSF + 
    LotArea + YearBuilt + GarageSF + YearRemodel + Bedroom + 
    LotFrontage + FullBath + Condition, data = AmesData2)
anova(reducedmodel1, fullmodel1)
```

#This adjustment impacted the model chosen by R's stepwise regression, leading to a model with only 13 predictors. However we observed that some of the predictors in this model (Fireplaces, Condition, and Fullbath) did not reach statistical significance at the 0.05 level. Therefore, we decided to conduct an F-test again, this time removing Fireplaces from the reduced model.


```{r}
summary(reducedmodel1)
vif(reducedmodel1)
```
#In this 10-predictor model with significant outliers removed, all predictors demonstrate statistical significance at the 0.05 level. The adjusted R squared stands at 0.8893, and none of the predictors exhibit VIF values exceeding 5, alleviating concerns about multicollinearity. Interestingly, the VIF of GroundSF, which initially approached 5 in the first model, notably decreased in this iteration.

### Part 3. Prediction for your "basic" model

Suppose that you are interested in a house in Ames that has the characteristics listed below. Construct a 95% confidence interval for the mean price of such houses.

A 2 story 11 room home, built in 1987 and remodeled in 1999 on a 21540 sq. ft. lot with 328 feet of road frontage. Overall quality is good (7) and condition is average (5). The quality and condition of the exterior are both good (Gd) and it has a poured concrete foundation. There is an 757 sq. foot basement that has excellent height, but is completely unfinished and has no bath facilities. Heating comes from a gas air furnace that is in excellent condition and there is central air conditioning. The house has 2432 sq. ft. of living space above ground, 1485 on the first floor and 947 on the second, with 4 bedrooms, 2 full and one half baths, and 1 fireplace. The 2 car, built-in garage has 588 sq. ft. of space and is average (TA) for both quality and construction. The only porches or decks is a 205 sq. ft. open porch in the front. 

```{r}
perfect_house <- data.frame(
  LotFrontage = 328,
  LotArea = 21540,
  Quality = 7,
  Condition = 5,
  YearBuilt = 1987,
  YearRemodel = 1999,
  BasementFinSF = 0,
  BasementSF = 757,
  BasementUnFinSF = 757,
  FirstSF = 1485,
  SecondSF = 947,
  GroundSF = 2432,
  FullBath = 2,
  HalfBath = 1,
  Bedroom = 4,
  TotalRooms = 11,
  Fireplaces = 1,
  GarageCars = 2,
  GarageSF = 588,
  OpenPorchSF = 205,
  ScreenPorchSF = 0,
  EnclosedPorchSF = 0
)

predict.lm(reducedmodel1, perfect_house, interval = 'confidence', level = 0.95)

```
#In this test, we can confidently assert that the true mean price for all houses with the specified characteristics lies between $268,887.8 and $304,887.3 with 95% confidence.
    
### Part 4: Find a “better" model:    
    
In addition to the quantitative predictors from Part 1, you may now consider models with:     

* Transformations of predictors. You can include functions of quantitative predictors. Probably best to use the I() notation so you don’t need to create new columns when you run the predictions for the test data. For example:      lm(Price~LotArea+I(LotArea^2)+sqrt(LotArea)+log(LotArea),... 
* Transformations of the response. You might address curvature or skewness in residual plots by transforming the response prices with a function like log(Price ), sqrt(Price), Price^2, etc..  These should generally not need the I( ) notation to make these adjustments.
* Combinations of variables. This might include for example creating a new variable which would count the total bathrooms in the house in a single predictor.  

Do not haphazardly use transformation on predictors, but examine the relationships between the predictors and response to determine when a transformation would be warranted. Again use multiple model selection methods to determine a best "better" model, but now with transformed variables as possible predictors in the model. You should determine useful transformations **prior** to using the model selection methods.

__Discuss the process that you used to transform the predictors and/or response__ so that you could use this process in the future on a new data set. This discussion will carry the bulk of your grade for this part.

#Upon reviewing the plots, we observed that several predictors, including YearRemodel, YearBuilt, BasementFinSF, BasementUnFinSF, GroundSF, and others, displayed an exponential relationship with Price. To address this pattern, a common strategy is to apply a natural logarithm transformation to the response variable. Consequently, we decided to log-transform Price in our model. This transformation notably improved the normality of residuals, as evidenced by the normal quantile plot.

#Additionally, we introduced a new variable representing the total porch square footage by summing OpenPorchSF, EnclosedPorchSF, and ScreenPorchSF. We believed that overall porch size would yield a stronger correlation with house price compared to analyzing individual porch types, especially considering that some houses feature a combined porch comprising various types.

#Lastly, we applied a square root transformation to LotArea and squared BasementUnFinSF to establish a more linear relationship with Price.

```{r}
AmesData2$TotalPorchSF <- AmesData2$OpenPorchSF + AmesData2$EnclosedPorchSF + AmesData2$ScreenPorchSF
AmesData2$sqrt_LotArea <- sqrt(AmesData2$LotArea)
AmesData2$sqr_BasementUnFinSF <- AmesData2$BasementUnFinSF^2

plot(Price ~ ., data = AmesData2)

#secondfloor <- HouseData2$SecondSF > 0
#HouseData2$SecondFloor <- secondfloor
#
Full <- lm(log(Price) ~ ., data = AmesData2)
MSE <- (summary(Full)$sigma)^2
none <- lm(log(Price) ~ 1, data = AmesData2)
step(none, scope = list(upper = Full), scale = MSE, trace = FALSE)


```

```{r}
mod3 <- lm(log(Price) ~ Quality + GroundSF + YearBuilt + sqrt_LotArea + 
    Condition + BasementSF + TotalPorchSF + Fireplaces + GarageCars + 
    sqr_BasementUnFinSF + YearRemodel + FullBath, data = AmesData2)
summary(mod3)
vif(mod3)

```


### Part 5. Residual analysis for your fancier model   

Repeat the residual analysis from Part 2 on your new model constructed in Part 4. A residual analysis was likely (hopefully) part of your process for determining your "better" model. That does not need to be fully repeated here. You should include any plots relevant to checking model conditions - with interpretations, as well as discussing whether any of the data cases are unusual with respect to standardized/studentized residuals. Make sure to indicate what specific criteria you are using to identify “unusual” points.

```{r}
plot(mod3)

#sum(abs(rstandard(newmodel)) > 3)
rstandard_data2 <- abs(rstandard(mod3)) > 3
#sum(abs(rstudent(newmodel)) > 2)
rstudent_data2 <- abs(rstudent(mod3)) > 2
AmesData3 <- AmesData2[!rstandard_data2, ]



AmesModFull3 <- lm(Price ~ ., AmesData2)
MSE3 <- (summary(AmesModFull)$sigma)^2
AmesModNone3 <- lm(Price ~1, AmesData2)
simple_model3 <- step(AmesModNone3, scale = MSE3, scope = list(upper = AmesModFull3), trace = FALSE)
summary(simple_model3)
```
#Upon examining the normal quantile plot, we observed that the residuals somewhat conform to a normal distribution, albeit with noteworthy extreme values at both ends. This implies that while the distribution of residuals possesses longer tails on both ends, there seems to be a heterogeneous distribution of both large and small residuals, suggesting a nuanced skewness. Notably, despite these outliers, the data aligns relatively well along a straight line, indicating a predominantly normal distribution, albeit with some deviations.

#Transitioning to the residuals vs. fitted plot, we discern a substantially straighter line compared to our initial model. This suggests a favorable fit of the current model to the data, with no apparent major issues or concealed trends within the dataset.

#Similarly, the scale-location plot presents a subtle curve, albeit less pronounced than observed in our original model. This, in conjunction with the relatively consistent spread evident in the residuals vs. fitted plot, indicates a mostly stable variance among the residuals.

#Finally, the residuals vs. leverage plot pinpointed observation number 294 as lying beyond Cook's Distance, identifying it as the sole significantly influential observation in the dataset, thus underscoring its unique impact on the model.

#Next, we examined the standardized and studentized residuals exceeding 3 and 2, respectively. We proceeded to remove observations surpassing the threshold of three in standardized residuals, ensuring that each of these observations also exceeded the studentized threshold of two.


```{r}
fullmodel1 <- lm(log(Price) ~ Quality + GroundSF + YearBuilt + sqrt_LotArea + 
    BasementSF + Condition + GarageSF + Fireplaces + YearRemodel + 
    TotalPorchSF + sqr_BasementUnFinSF + Bedroom + FullBath + 
    GarageCars + LotArea, data = AmesData3)
reducedmodel1 <- lm(log(Price) ~ Quality + GroundSF + YearBuilt + sqrt_LotArea + 
    BasementSF + Condition + GarageSF + Fireplaces + YearRemodel + 
    TotalPorchSF + sqr_BasementUnFinSF + Bedroom, data = AmesData3)
anova(fullmodel1, reducedmodel1)
```

#Following the removal of outliers, we employed stepwise regression to seek an improved model from the refined dataset. However, it emerged that four predictors did not attain statistical significance at the 0.05 level. Consequently, we opted to reapply the F-test. After some experimentation, recognizing that eliminating all four predictors would yield a very low p-value, we opted for a reduced model by removing three predictors. Setting H0 as the reduced model and HA as the full model, the resulting F-test yielded a p-value of 0.06185, exceeding 5. Therefore, we failed to reject the null hypothesis, retaining our reduced model.

```{r}
finished <- reducedmodel1
summary(finished)
vif(finished)
```
#Our ultimate model comprises 12 predictor variables, all of which exhibit statistical significance at the 0.05 level. Additionally, none of these variables demonstrate problematic VIF values (> 5), reassuring us regarding multicollinearity concerns.

### Part 6. Prediction for your "better" model  

Again suppose that you are interested in a house in Ames that has the characteristics listed below. Construct a 95% confidence interval for the mean price of such houses. Make sure that your interval is in dollars (in $1,000’s) and not transformed units!

A 2 story 11 room home, built in 1987 and remodeled in 1999 on a 21540 sq. ft. lot with 328 feet of road frontage. Overall quality is good (7) and condition is average (5). The quality and condition of the exterior are both good (Gd) and it has a poured concrete foundation. There is an 757 sq. foot basement that has excellent height, but is completely unfinished and has no bath facilities. Heating comes from a gas air furnace that is in excellent condition and there is central air conditioning. The house has 2432 sq. ft. of living space above ground, 1485 on the first floor and 947 on the second, with 4 bedrooms, 2 full and one half baths, and 1 fireplace. The 2 car, built-in garage has 588 sq. ft. of space and is average (TA) for both quality and construction. The only porches or decks is a 205 sq. ft. open porch in the front. 

Compare and contrast the two predictions made for the mean price using your "basic" and "better' models.

```{r}
one_more_house <- data.frame(TotalRooms = 11, 
                             YearBuilt = 1987, 
                             YearRemodel = 1999, 
                             LotArea = 21540, 
                             LotFrontage = 328, 
                             Quality = 7, 
                             Condition = 5, 
                             BasementUnFinSF = 757, 
                             BasementFinSF = 0, 
                             BasementSF = 757, 
                             GroundSF = 2432, 
                             FirstSF = 1485, 
                             SecondSF = 947, 
                             Bedroom = 4, 
                             FullBath = 2, 
                             HalfBath = 1, 
                             Fireplaces = 1, 
                             GarageSF = 588, 
                             GarageCars = 2, 
                             OpenPorchSF = 205, 
                             EnclosedPorchSF = 0, 
                             ScreenPorchSF = 0, 
                             sqrt_LotArea = sqrt(21540), 
                             TotalPorchSF = 205, 
                             sqr_BasementUnFinSF = 573049)

exp(predict.lm(finished, one_more_house, interval = 'confidence', level = 0.95))
```

#Thus, we can assert with 95% confidence that the true mean price for all houses possessing the aforementioned characteristics falls within the range of $252,379.90 and $269,156.50.



#These two predictions were surprising because the intervals did not overlap whatsoever. I believe this discrepancy is highly attributed to the removal of outliers. In our view, this final model is  superior to the original one when attempting to predict house prices, as outliers had a significant impact on the original model. When dealing with housing data historical factors can creat massive outlyers and by reoming those outlyers the model more tightly resenmbes what the average person could expact when buying a home.
